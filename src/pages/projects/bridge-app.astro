---
import CaseStudyLayout from "../../layouts/CaseStudyLayout.astro";
import { Image } from "astro:assets";
import { bridgeapp } from "../../utils/images";
import CodeBlock from "../../components/CodeBlock.astro";

const sidebarItems = [
  { text: "Overview", href: "#overview" },
  { text: "Context", href: "#context" },
  { text: "Development", href: "#development" },
];
---

<CaseStudyLayout
  title="BRIDGE Dashboard"
  subtitle="ASL Translation Tool - October 2024"
  pageTitle="BRIDGE Dashboard - Liam Sullivan"
  sidebarItems={sidebarItems}
  projectRole={["Full-Stack Development", "UI + UX Design"]}
  team={["FACETLab @ University of Pittsburgh"]}
  tools={["Svelte", "TypeScript", "OpenAI API", "Vite"]}
  timeline={["3 Months"]}
  githubLink="https://github.com/liambsulliva/bridge-dashboard"
>
  <Image
    src={bridgeapp}
    alt="BRIDGE Dashboard"
    class="mx-auto max-h-[700px] w-auto rounded-xl"
    loading="eager"
    slot="hero"
  />

  <div class="case-study-layout">
    <section class="pb-10 pt-10 md:flex md:gap-20" id="overview">
      <h3 class="text-2xl font-semibold text-white">Overview</h3>
      <div>
        <p>
          FACETLab is a research organization at the University of Pittsburgh
          that specializes in designing adaptive educational technologies for
          youth in a variety of disciplines. Artificial Intelligence (AI), Human
          Computer Interaction (HCI), and Educational Psychology are a few of
          their key focus areas.
        </p>
        <br />
        <p>
          The BRIDGE project that I worked with narrows this scope down to
          members of the deaf and hard of hearing community. Specifically,
          American Sign Language (ASL) learners. It targets members of this
          community engaging in STEM learning, as members of this group are
          historically underrepresented in these fields.
        </p>
      </div>
    </section>

    <section id="context">
      <h3 class="!mb-3 text-2xl font-semibold text-white">Context</h3>
      <p>
        There are a variety of reasons that ASL learners struggle with STEM
        education. For one, there is a lack of consistency between signage for
        these complex terms. One option is fingerspelling, where students
        physically sign every single letter in the word, but this process is
        slow and inefficient for explaining jargon-heavy topics. Students may
        also use their own made-up signs, which ends up causing problems when
        different students use differing signage from one another.
      </p>
      <br />
      {
        /* <div class="card-container">
        <div class="card">
          <svg
            class="mx-auto my-4"
            xmlns="http://www.w3.org/2000/svg"
            width="7rem"
            height="7rem"
            viewBox="0 0 24 24"
            ><path
              fill="white"
              d="M21 3H3c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h18c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2m0 16H3V5h18zM11 15h2v-2h2v-2h-2V9h-2v2H9v2h2z"
            ></path></svg
          >
          <h4 class="text-xl font-medium text-white">
            Sequential Generation Queue
          </h4>
          <p>
            Missing words are processed one at a time through a modal queue
            system. This prevents overwhelming the OpenAI API with simultaneous
            requests and gives users control over which signs to generate versus
            which to skip and fingerspell instead.
          </p>
        </div>
        <div class="card">
          <svg
            class="mx-auto my-2"
            xmlns="http://www.w3.org/2000/svg"
            width="8rem"
            height="8rem"
            viewBox="0 0 24 24"
            ><path
              fill="white"
              d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10s10-4.48 10-10S17.52 2 12 2m-2 15l-5-5l1.41-1.41L10 14.17l7.59-7.59L19 8z"
            ></path></svg
          >
          <h4 class="text-xl font-medium text-white">
            Dynamic Library Updates
          </h4>
          <p>
            After generating a new sign, the system writes the base64-encoded
            image to the local filesystem and dynamically reloads it into the
            sign library store. This ensures newly generated signs are
            immediately available without requiring an app restart or manual
            refresh.
          </p>
        </div>
        <div class="card">
          <svg
            class="mx-auto my-2"
            xmlns="http://www.w3.org/2000/svg"
            width="7rem"
            height="8rem"
            viewBox="0 0 24 24"
            ><path
              fill="white"
              d="M20 6h-2.18c.11-.31.18-.65.18-1a2.996 2.996 0 0 0-5.5-1.65l-.5.67l-.5-.68C10.96 2.54 10.05 2 9 2C7.34 2 6 3.34 6 5c0 .35.07.69.18 1H4c-1.11 0-1.99.89-1.99 2L2 19c0 1.11.89 2 2 2h16c1.11 0 2-.89 2-2V8c0-1.11-.89-2-2-2m-5-2c.55 0 1 .45 1 1s-.45 1-1 1s-1-.45-1-1s.45-1 1-1M9 4c.55 0 1 .45 1 1s-.45 1-1 1s-1-.45-1-1s.45-1 1-1m11 15H4v-2h16zm0-5H4V8h5.08L7 10.83L8.62 12L12 7.4l3.38 4.6L17 10.83L14.92 8H20z"
            ></path></svg
          >
          <h4 class="text-xl font-medium text-white">Visual Hierarchy</h4>
          <p>
            The SignDisplay component distinguishes between word-level signs
            (larger with yellow ring styling) and letter-level fingerspelling
            (smaller). Visual separators between words improve readability,
            making it clear where one word ends and another begins in the ASL
            translation.
          </p>
        </div>
      </div>
      */
      }
      <p>
        Since deaf students are so underrepresented in these fields, traditional
        support structures such as office hours and professorial guidance are
        not structured to handle sign-based learning. Plaforms like Zoom rely on
        audio-based cues in order to determine who is speaking, which makes the
        platform completely inaccessible for ASL learners.
      </p>
      <br />
      <p>
        The hypothesis of this research project is that AI-driven technology can
        be leveraged to substantially improve collaboration and learning
        outcomes among members of the ASL community. The project seeks to unify
        signers in a landscape where semantics are fragmented and commonly
        misunderstood.
      </p>
      <br />
      <p>
        The project also incorporates Augmented Reality (AR) technology to
        support ASL learners in a tangible way that is difficult to replicate
        with other technologies. Since signage is so spatially dependent, it is
        critical that learners see their interlocutors clearly and can identify
        the positioning of their hands relative to their face. AR uniquely
        affords this kind of use, which makes it an excellent fit for this form
        of research.
      </p>
      <br />
      <p>
        To gain a wholistic understanding of the community and their needs,
        experts in ASL learning, such as <a
          href="mailto:athena.s.willis@gmail.com">Dr. Athena Willis</a
        >, have been onboarded to support the team and its goals. Professionals
        in the fields of HCI and AI, namely <a href="mailto:biehl@pitt.edu"
          >Dr. Jacob Biehl</a
        > and <a href="mailto:kezarlee@gmail.com">Dr. Lee Kezar</a>
        , have also been brought on to support this mission and ensure that interactions
        between signers remain relatively intuitive.
      </p>
    </section>
    <br />
    <section id="development">
      <h3 class="!mb-3 text-2xl font-semibold text-white">Development</h3>
      <p>
        Placing AI-powered signage at the forefront of my application meant that
        the entire user flow was built around the process of discovery. Entering
        an English query in a search window, seeing it spelled out for you in
        ASL, and generating appropriate signage along the way with OpenAI’s
        <code>gpt-image-1</code> model. If users found a good ASL sign representation
        of a word they are learning from an external source, they are also given the
        means to upload their own images to be analyzed through OpenAI’s Vision API.
      </p>
      <br />
      <p>
        Whether images are manually uploaded or AI-generated, users are
        incentivized to build up a library of images to use for their future
        queries. As the user interacts with the application, they develop new
        signage for new words. This signage helps them form additional phrases
        and gradually fill gaps in the image library, and thus in their
        knowledge.
      </p>
      <br />
      <p>
        To incentivize users to fill their library with words, there is a “word
        map” tab that clusters words by semantic similarity and physically links
        them if they are used in the same phrase. Additionally, a variable
        progress bar is displayed at the top to add an element of gamification
        to the engagement with the map. Users can use this section of the
        interface to zoom out and obtain a tangible view of their progress and
        understanding.
      </p>
      <br />
      <p>
        All signage is stored in a globally accessible data repository that is
        called across the application (<code>signLibrary.ts</code>). Basic ASL
        lettering (a-z) is pre-loaded into the application to handle
        fingerspelling, but if word-level signs exist in the library, the
        application will prioritize those instead. To detect which words are
        relevant enough to be flagged for AI generation, the Natural Language
        Processing (NLP) JavaScript library compromise tokenizes the string and
        separates out nouns, adjectives, verbs, and adverbs.
      </p>
      <CodeBlock
        code={`// Extract content words using NLP (compromise)
function extractContentWords(text: string): Set<string> {
	const doc = nlp(text);
	const contentWords = new Set<string>();
	const wordPattern = /^[a-z]+(?:-[a-z]+)?$/;

	doc.nouns().forEach((noun: CompromiseMatch) => {
		const word = noun.text().toLowerCase().trim();
		if (word && wordPattern?.test(word)) {
			contentWords.add(word);
			//console.log('Extracted noun:', word);
		}
	});

	doc.verbs().forEach((verb: CompromiseMatch) => {
		const word = verb.text().toLowerCase().trim();
		if (word && wordPattern?.test(word)) {
			contentWords.add(word);
			//console.log('Extracted verb:', word);
		}
	});

	doc.adjectives().forEach((adj: CompromiseMatch) => {
		const word = adj.text().toLowerCase().trim();
		if (word && wordPattern?.test(word)) {
			contentWords.add(word);
			//console.log('Extracted adjective:', word);
		}
	});
  
    //...

    return contentWords;
}`}
        language="typescript"
      />
      <p>
        If a given word falls into any one of these categories and doesn’t
        already have an associated image reference in the library, it is marked
        for generation. Additionally, there is a base dictionary Set that is
        referenced to check for common phrases like “thank you”, “hello”, and
        “goodbye”.
      </p>
      <br />
      <p>
        When a word is detected and a user approves an AI-generated sign, a POST
        request is sent through a SvelteKit API route (<code
          >/api/generate-sign</code
        >) to be processed on the server. Using the stored OpenAI API key, the
        service generates a 1024 x 1024 image to be sent back to the client to
        be displayed. In the meantime, the user sees a loading indicator while
        the isGenerating state is set to true.
      </p>
      <CodeBlock
        code={`// Compose the prompt dynamically based on the word
const prompt = \`
American Sign Language (ASL) sign for the word "\${word}, in a black and white illustrative style
with a clear hand gesture on a white background. Do not display any text in the final result.\`;

console.log('[STEP] Prompt sent to OpenAI:', prompt);

const result = await openai.images.generate({
  model: 'gpt-image-1',
  prompt,
  size: '1024x1024',
  n: 1
});

console.log('[STEP] Raw OpenAI image result:', !!result && result.data && result.data[0]);`}
        language="typescript"
      />
      <p>
        User uploads also go through an API route (<code
          >/api/recognize-sign</code
        >) to be processed by OpenAI’s Vision API. Once that is complete, the
        Sharp library is invoked to convert the uploaded image into a PNG file
        and optimize it to save space in the library. This allows users to
        upload a diverse range of image formats without needing to worry about
        pre-compressing their images.
      </p>
    </section>
  </div>
</CaseStudyLayout>

<style>
  a {
    color: var(--header-color);
    text-decoration: underline;
    text-decoration-color: var(--header-color);
    text-underline-offset: 2px;
    font-weight: 600;
    display: inline-block;
    position: relative;
  }

  @media (hover: hover) {
    a {
      text-decoration: none;
    }
    a:after {
      bottom: 0;
      content: "";
      height: 2px;
      left: 50%;
      position: absolute;
      background: var(--header-color);
      transition:
        width 0.2s ease 0s,
        left 0.2s ease 0s;
      width: 0;
    }
    a:hover:after {
      width: 100%;
      left: 0;
    }
  }

  code {
    background-color: #1e1e1e;
    color: #d4d4d4;
    font-family: "Martian Mono", "Courier New", Courier, monospace;
    font-size: 0.875rem;
    padding: 0.125rem 0.375rem;
    border-radius: 4px;
    border: 1px solid #333;
    white-space: nowrap;
  }

  @media (max-width: 768px) {
    code {
      font-size: 0.75rem;
    }
  }

  :global(.case-study-layout) .card-container {
    display: flex;
    flex-wrap: wrap;
    gap: 1rem;
    justify-content: space-between;
  }

  :global(.case-study-layout) .card {
    background-color: var(--card-color);
    border: 1px solid #333;
    border-radius: 0.75rem;
    padding: 1.5rem;
    width: calc(33.33% - 0.67rem);
    margin-bottom: 1rem;
  }

  :global(.case-study-layout) .card h4 {
    margin-bottom: 0.5rem;
  }

  @media (max-width: 768px) {
    :global(.case-study-layout) .card {
      width: 100%;
    }
  }
</style>
